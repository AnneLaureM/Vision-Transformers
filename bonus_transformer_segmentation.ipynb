{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Course/TP \u2014 Transformer-based Segmentation for Astrophysics\nGenerated on 2025-10-28T18:20:06.571472Z\n\n> A complete, hands-on notebook that teaches Transformer-based image segmentation on astro-like data. Includes theory, architecture choices, code, metrics, and visual diagnostics. No internet needed."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## Learning Outcomes\n\nBy the end of this TP you will be able to:\n\n1. Generate and inspect an **astro-like synthetic segmentation dataset** (galaxies on noisy backgrounds).\n2. Implement a **Transformer-based segmentation model** (lite SegFormer-style encoder + simple decoder).\n3. Understand **patch/overlap embeddings**, **self-attention**, **positional encodings**, and **token mixing** in a segmentation setting.\n4. Train with **Dice + BCE** loss, monitor **IoU** and **Dice**, and visualize qualitative predictions.\n5. Tune key hyperparameters (patch size, embedding dimension, number of heads/layers, learning rate, augmentations).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## Context\n\nAstronomical surveys often require **pixel-wise segmentation** to separate sources from background, delineate galaxy disks, bulges, arms, or to flag artifacts. Transformers have become competitive for dense prediction tasks through encoder-decoder designs such as **SegFormer** and **Mask2Former**. This notebook builds a **minimal working prototype**, optimized for clarity over maximum performance.\n\nIf you later want to plug in **real data**, you can adapt the dataset class to read FITS or PNG images and load masks prepared by your pipeline (e.g., SExtractor, human annotations).\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Environment Check & Imports"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\nimport os, math, random, time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n\n# Reproducibility\nSEED = 1234\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 2) Synthetic Astro-like Dataset\n\nWe create simple grayscale scenes (H\u00d7W) with:\n- Background noise (Gaussian + Poisson-like mix)\n- A few **galaxy-like blobs** (elliptical Gaussian profiles)\n- Optional **bar/arm hints** with simple parametric curves\n- Final **binary mask**: 1 for galaxy pixels, 0 for background\n\nThis is intentionally simple but surprisingly effective for practicing segmentation.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\nH, W = 128, 128           # image size\nN_TRAIN, N_VAL, N_TEST = 800, 120, 120\nBATCH_SIZE = 16\n\ndef draw_elliptical_gaussian(img, msk, cx, cy, a, b, theta, peak):\n    # Elliptical Gaussian blob centered at (cx,cy) with axes a,b and angle theta\n    yy, xx = np.mgrid[0:H, 0:W]\n    x0 = xx - cx\n    y0 = yy - cy\n    ct, st = np.cos(theta), np.sin(theta)\n    xr =  ct * x0 + st * y0\n    yr = -st * x0 + ct * y0\n    g = peak * np.exp(-0.5 * ((xr / (a+1e-6))**2 + (yr / (b+1e-6))**2))\n    img += g\n    msk[:] = np.maximum(msk, (g > (0.1*peak)).astype(np.float32))\n\ndef add_bar(img, cx, cy, length, width, theta, amp):\n    yy, xx = np.mgrid[0:H, 0:W]\n    x0 = xx - cx\n    y0 = yy - cy\n    ct, st = np.cos(theta), np.sin(theta)\n    xr =  ct * x0 + st * y0\n    yr = -st * x0 + ct * y0\n    bar = np.exp(-0.5*((yr/width)**2)) * (np.abs(xr) < length).astype(np.float32)\n    img += amp * bar\n\ndef add_spiral_hint(img, cx, cy, turns, amp):\n    # Very rough spiral hint using log-spiral radius\n    yy, xx = np.mgrid[0:H, 0:W]\n    r = np.sqrt((xx-cx)**2 + (yy-cy)**2) + 1e-6\n    ang = np.arctan2(yy-cy, xx-cx)\n    # create ridges in angle-radius space\n    k = turns * 0.5\n    ridge = np.sin(k * np.log(r+1.0) + 3*ang)\n    ridge = (ridge > 0.9).astype(np.float32)\n    img += amp * ridge\n\ndef make_sample():\n    img = np.zeros((H, W), dtype=np.float32)\n    msk = np.zeros((H, W), dtype=np.float32)\n    # background\n    img += 0.02*np.random.randn(H, W).astype(np.float32)\n    img += np.random.poisson(1.0, size=(H, W)).astype(np.float32) * 0.001\n\n    # number of galaxies\n    n_obj = np.random.randint(1, 4)\n    for _ in range(n_obj):\n        cx = np.random.uniform(20, W-20)\n        cy = np.random.uniform(20, H-20)\n        a  = np.random.uniform(4, 12)\n        b  = np.random.uniform(3, 10)\n        theta = np.random.uniform(0, np.pi)\n        peak = np.random.uniform(0.7, 1.5)\n        draw_elliptical_gaussian(img, msk, cx, cy, a, b, theta, peak)\n\n        if np.random.rand() < 0.4:\n            add_bar(img, cx, cy, length=np.random.uniform(6, 14),\n                    width=np.random.uniform(1.5, 3.5), theta=theta, amp=0.2)\n\n        if np.random.rand() < 0.3:\n            add_spiral_hint(img, cx, cy, turns=np.random.uniform(1.0, 2.5), amp=0.1)\n\n    # normalization to [0,1]\n    img -= img.min()\n    img /= (img.max() + 1e-6)\n    return img.astype(np.float32), msk.astype(np.float32)\n\n# Quick sanity check sample\nimg, msk = make_sample()\nprint(\"Sample pixel stats:\", img.min(), img.max(), msk.mean())\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\n# Dataset & DataLoader\nclass ToyAstroSegDataset(Dataset):\n    def __init__(self, n_samples):\n        self.n = n_samples\n    def __len__(self):\n        return self.n\n    def __getitem__(self, idx):\n        img, msk = make_sample()\n        img = img[None, ...]   # add channel: (1, H, W)\n        return torch.from_numpy(img), torch.from_numpy(msk)\n\ntrain_ds = ToyAstroSegDataset(N_TRAIN)\nval_ds   = ToyAstroSegDataset(N_VAL)\ntest_ds  = ToyAstroSegDataset(N_TEST)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nlen(train_ds), len(val_ds), len(test_ds)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n### Visualization\n\nAlways look at your data. Here we plot a few images with their masks.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\ndef show_batch(dataset, n=6):\n    import math\n    n = min(n, len(dataset))\n    cols = 3\n    rows = int(math.ceil(n/cols))\n    plt.figure(figsize=(cols*4, rows*4))\n    for i in range(n):\n        img, msk = dataset[i]\n        plt.subplot(rows, cols, i+1)\n        plt.imshow(img[0].numpy())\n        plt.title(\"Image (grayscale)\")\n        plt.axis(\"off\")\n    plt.show()\n\n    plt.figure(figsize=(cols*4, rows*4))\n    for i in range(n):\n        img, msk = dataset[i]\n        plt.subplot(rows, cols, i+1)\n        plt.imshow(msk.numpy())\n        plt.title(\"Mask\")\n        plt.axis(\"off\")\n    plt.show()\n\nshow_batch(train_ds, n=6)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 3) Transformer-based Segmentation Model\n\nWe implement a **lightweight SegFormer-like** architecture:\n\n- **Overlapping Patch Embedding**: A small convolution projects the input into tokens with spatial stride (patch size) and overlap, improving local continuity.\n- **Transformer Encoder Blocks**: Stacks of self-attention + MLP (with GELU), LayerNorm pre-norm, residual connections.\n- **Positional Encoding**: 2D learnable positional embeddings added to token features.\n- **Decoder**: Simple upsampling head that uses a sequence-to-image reshape and a few conv layers to output a dense mask.\n\n### Key Parameters\n\n- `patch_size` (stride): controls downsampling. Larger stride reduces memory but loses fine detail.\n- `embed_dim`: token channel dimension. Higher improves capacity but costs memory.\n- `depth`: number of Transformer layers.\n- `num_heads`: attention heads. More heads give more subspace mixing but add compute.\n- `mlp_ratio`: hidden size of MLP (`mlp_dim = embed_dim * mlp_ratio`).\n\nWe keep this model compact so it trains fast on CPU/GPU.\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\nclass OverlapPatchEmbed(nn.Module):\n    def __init__(self, in_ch=1, embed_dim=64, patch_size=8, overlap=0.5):\n        super().__init__()\n        stride = patch_size\n        k = int(patch_size + patch_size*overlap)\n        if k % 2 == 0:\n            k += 1  # ensure odd kernel for centered receptive field\n        padding = k // 2\n        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=k, stride=stride, padding=padding)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.patch_size = patch_size\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        x = self.proj(x)  # (B, E, H', W')\n        B, E, Hp, Wp = x.shape\n        x = x.flatten(2).transpose(1, 2)  # (B, N, E), N = Hp*Wp\n        x = self.norm(x)\n        return x, (Hp, Wp)\n\nclass MLP(nn.Module):\n    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n        super().__init__()\n        hidden = int(dim * mlp_ratio)\n        self.fc1 = nn.Linear(dim, hidden)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden, dim)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads=4, mlp_ratio=4.0, attn_drop=0.0, drop=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=attn_drop, batch_first=True)\n        self.drop_path1 = nn.Dropout(drop)\n\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = MLP(dim, mlp_ratio=mlp_ratio, drop=drop)\n        self.drop_path2 = nn.Dropout(drop)\n\n    def forward(self, x):\n        # Self-attention\n        x_res = x\n        x = self.norm1(x)\n        attn_out, _ = self.attn(x, x, x, need_weights=False)\n        x = x_res + self.drop_path1(attn_out)\n\n        # MLP\n        x_res = x\n        x = self.norm2(x)\n        x = x_res + self.drop_path2(self.mlp(x))\n        return x\n\nclass TinySegFormer(nn.Module):\n    def __init__(self, in_ch=1, embed_dim=96, depth=4, num_heads=4, mlp_ratio=3.0,\n                 patch_size=8, overlap=0.5, num_classes=1):\n        super().__init__()\n        self.embed = OverlapPatchEmbed(in_ch, embed_dim, patch_size, overlap)\n        self.pos_emb = None  # initialized after first forward (depends on H'W')\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio)\n            for _ in range(depth)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n        self.num_classes = num_classes\n\n        # Decoder: simple conv head after reshaping back to (H', W')\n        self.decoder = nn.Sequential(\n            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Conv2d(embed_dim, embed_dim//2, kernel_size=3, padding=1),\n            nn.GELU(),\n            nn.Conv2d(embed_dim//2, num_classes, kernel_size=1),\n        )\n\n        self.patch_size = patch_size\n\n    def forward(self, x):\n        B = x.shape[0]\n        tokens, (Hp, Wp) = self.embed(x)        # (B, N, E)\n        N, E = tokens.shape[1], tokens.shape[2]\n\n        # create 2D learned positional embeddings if not set\n        if self.pos_emb is None or self.pos_emb.shape[1] != N:\n            self.pos_emb = nn.Parameter(torch.zeros(1, N, E, device=x.device))\n            nn.init.trunc_normal_(self.pos_emb, std=0.02)\n\n        z = tokens + self.pos_emb               # add positional info\n        for blk in self.blocks:\n            z = blk(z)\n        z = self.norm(z)                        # (B, N, E)\n\n        z = z.transpose(1, 2).reshape(B, E, Hp, Wp)  # back to spatial\n\n        # upsample to full resolution\n        scale = self.patch_size\n        z = F.interpolate(z, scale_factor=scale, mode=\"bilinear\", align_corners=False)\n\n        logits = self.decoder(z)                # (B, num_classes, H, W)\n        return logits\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n### Why Overlapping Patches?\n\nPure non-overlapping patches can create **block artifacts** and miss local continuity. Overlapping patch embedding combines small convolutions with stride to produce tokens that share neighbors. This improves segmentation edges and stability without heavy computation.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Losses & Metrics: Dice, BCE, IoU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\ndef dice_loss(pred, target, eps=1e-6):\n    # pred: logits (B,1,H,W) -> apply sigmoid inside\n    prob = torch.sigmoid(pred)\n    num = 2 * (prob * target).sum(dim=(1,2,3))\n    den = (prob + target).sum(dim=(1,2,3)) + eps\n    dice = 1 - (num / den)\n    return dice.mean()\n\ndef bce_loss(pred, target):\n    return F.binary_cross_entropy_with_logits(pred, target)\n\ndef combo_loss(pred, target, w_dice=0.6, w_bce=0.4):\n    return w_dice * dice_loss(pred, target) + w_bce * bce_loss(pred, target)\n\n@torch.no_grad()\ndef iou_score(pred, target, thresh=0.5, eps=1e-6):\n    prob = torch.sigmoid(pred)\n    pred_mask = (prob >= thresh).float()\n    inter = (pred_mask * target).sum(dim=(1,2,3))\n    union = (pred_mask + target - pred_mask*target).sum(dim=(1,2,3)) + eps\n    iou = (inter / union).mean().item()\n    return iou\n\n@torch.no_grad()\ndef dice_coeff(pred, target, thresh=0.5, eps=1e-6):\n    prob = torch.sigmoid(pred)\n    pred_mask = (prob >= thresh).float()\n    num = 2 * (pred_mask * target).sum(dim=(1,2,3))\n    den = (pred_mask + target).sum(dim=(1,2,3)) + eps\n    return (num / den).mean().item()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 5) Training Loop\n\nWe use AdamW, a modest learning rate, and track validation IoU/Dice. Tune:\n\n- `embed_dim`, `depth`, `num_heads`: capacity\n- `patch_size`: resolution vs compute\n- `lr`, `weight_decay`, `epochs`: optimization\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\nmodel = TinySegFormer(in_ch=1, embed_dim=128, depth=4, num_heads=4, mlp_ratio=3.0,\n                      patch_size=8, overlap=0.5, num_classes=1).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\nEPOCHS = 10\n\ndef train_one_epoch(loader):\n    model.train()\n    total_loss = 0.0\n    for img, msk in loader:\n        img = img.to(device)\n        msk = msk.to(device).unsqueeze(1)  # (B,1,H,W)\n        optimizer.zero_grad()\n        logits = model(img)\n        loss = combo_loss(logits, msk)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * img.size(0)\n    return total_loss / len(loader.dataset)\n\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    total_loss = 0.0\n    iou, dice = [], []\n    for img, msk in loader:\n        img = img.to(device)\n        msk = msk.to(device).unsqueeze(1)\n        logits = model(img)\n        loss = combo_loss(logits, msk)\n        total_loss += loss.item() * img.size(0)\n        iou.append(iou_score(logits, msk))\n        dice.append(dice_coeff(logits, msk))\n    return total_loss / len(loader.dataset), float(np.mean(iou)), float(np.mean(dice))\n\ntrain_hist = {\"train_loss\": [], \"val_loss\": [], \"val_iou\": [], \"val_dice\": []}\nfor epoch in range(1, EPOCHS+1):\n    tr = train_one_epoch(train_loader)\n    vl, viou, vdice = evaluate(val_loader)\n    train_hist[\"train_loss\"].append(tr)\n    train_hist[\"val_loss\"].append(vl)\n    train_hist[\"val_iou\"].append(viou)\n    train_hist[\"val_dice\"].append(vdice)\n    print(f\"Epoch {epoch:02d} | train {tr:.4f} | val {vl:.4f} | IoU {viou:.3f} | Dice {vdice:.3f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Curves: Loss / IoU / Dice"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\n# One plot per figure, matplotlib only, no specific colors\nplt.figure()\nplt.plot(train_hist[\"train_loss\"], label=\"train loss\")\nplt.plot(train_hist[\"val_loss\"], label=\"val loss\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training vs Validation Loss\"); plt.legend(); plt.show()\n\nplt.figure()\nplt.plot(train_hist[\"val_iou\"], label=\"val IoU\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"IoU\"); plt.title(\"Validation IoU\"); plt.legend(); plt.show()\n\nplt.figure()\nplt.plot(train_hist[\"val_dice\"], label=\"val Dice\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Dice\"); plt.title(\"Validation Dice\"); plt.legend(); plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Test Evaluation & Qualitative Results"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\ntest_loss, test_iou, test_dice = evaluate(test_loader)\nprint(f\"Test | loss {test_loss:.4f} | IoU {test_iou:.3f} | Dice {test_dice:.3f}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "\n@torch.no_grad()\ndef show_predictions(loader, n_batches=2):\n    model.eval()\n    shown = 0\n    for img, msk in loader:\n        img = img.to(device)\n        logits = model(img)\n        prob = torch.sigmoid(logits).cpu().numpy()\n        img = img.cpu().numpy()\n        msk = msk.numpy()\n        B = img.shape[0]\n        for b in range(B):\n            if shown >= n_batches * BATCH_SIZE:\n                return\n            plt.figure()\n            plt.imshow(img[b,0])\n            plt.title(\"Input image\")\n            plt.axis(\"off\")\n            plt.show()\n\n            plt.figure()\n            plt.imshow(msk[b])\n            plt.title(\"Ground truth mask\")\n            plt.axis(\"off\")\n            plt.show()\n\n            plt.figure()\n            plt.imshow((prob[b,0] >= 0.5).astype(np.float32))\n            plt.title(\"Prediction (thresholded)\")\n            plt.axis(\"off\")\n            plt.show()\n\n            shown += 1\n\nshow_predictions(test_loader, n_batches=1)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 7) Advanced Topics & Extensions\n\n1. **Multi-class Segmentation**: Change `num_classes` to K and use `CrossEntropyLoss` with one-hot targets for Dice (or per-class Dice).\n2. **Pyramid Features**: Build multiple stages with increasing stride (e.g., 4/8/16) and a lightweight decoder that fuses multiscale features.\n3. **Relative Positional Bias**: Replace absolute positional embeddings with relative biases to better generalize to varying sizes.\n4. **Data Augmentation**: Random rotations, flips, elastic deformations. For astronomy, prefer intensity-preserving transforms.\n5. **Mixed Precision**: Use `torch.cuda.amp` for faster training on GPUs.\n6. **Regularization**: Stochastic depth, attention dropout, spatial dropout.\n7. **Real FITS Data**: Replace the dataset with FITS readers. Normalize by exposure time, handle bad pixels with masks, and consider PSF variations.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 8) Key Hyperparameters: Practical Guide\n\n- `patch_size`: start at 8 or 4 for 128\u00d7128 images. Smaller captures finer edges but increases tokens.\n- `embed_dim`: 64\u2013256 is common for small problems. Larger boosts accuracy at compute cost.\n- `depth`: 4\u201312 layers depending on capacity and data size.\n- `num_heads`: 2\u20138 typically. More heads may help complex textures.\n- `mlp_ratio`: 3\u20134 is a good default.\n- `loss`: Dice + BCE is robust for class imbalance common in astronomy (small sources vs large background).\n- `lr`: 1e-3 to 3e-4 with AdamW works well; watch validation metrics to pick a schedule.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n## 9) Checklist for Real Datasets\n\n- [ ] Data loading (FITS/PNG), normalization, bad-pixel masking\n- [ ] Train/val/test split respecting fields/targets\n- [ ] Metric selection aligned to science goals (e.g., high recall of faint sources)\n- [ ] Calibration of predicted masks if used for photometry\n- [ ] Uncertainty estimation (e.g., MC dropout) for downstream vetting\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}